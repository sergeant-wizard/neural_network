## 今回の目標
[前回](https://github.com/sergeant-wizard/neural_network/tree/master/back_propagation_20150630)は
2層のネットワークであったが、
今回は3層のネットワークでより自由度の高い学習が可能であるかを検証する。
具体的には、教師あり学習により恒等写像の実現性を検証する。

## 前回との差分
- 基本的に入力/出力の差分のみ。
- `LastLayer`を導入
- 収束の様子がわかるように残差を導入
- 残差をgnuplotのコマンドで出力できるようにする
- 前回は重みの初期値を全成分同じ値にしていたが、これだと各節点につき対象な学習となってしまい問題が生じた。
  このため、今回は重みの各成分を0から1の間の乱数にした。ただし、実行の度に結果が変わるのは好ましくないので、
  乱数のseedを固定。
- 入力層：2節点、中間層：3節点、出力層：2節点
- 出力層の活性化関数を恒等写像に修正

## 結果
```
input = output = [
  +1
  -1
  -1
  +1
  +1
  +1
]
```
としたところ、残差(`(input - output)^2`)が以下の通り収束、
ネットワークが想定通りの学習をすることがわかった。

![residue](https://github.com/sergeant-wizard/neural_network/blob/master/back_propagation_20150701/residue.png)

FEMならこの規模の問題ならもっと早く収束するイメージであるが、ニューラルネットワークの特性上そこまで早くないようである。

