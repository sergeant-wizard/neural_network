# MLPシリーズ「深層学習」第4章の実装

## なぜ4章からなのか
2章はおもしろい結果を出すとこまでいかない。
3章は本書に書いてある通り、複雑な問題に適用できない。
4章を出発点とすればこの後の章にでてくる多くの問題に応用できそうだから。
4.1 - 4.3説の話は4.4節で一般化されるので、4.4節から実装する。

## 基本的な方針
まずはスケーラビリティがなくても、とにかく簡単に手っ取り早く実装し、
単純な例で期待通りの挙動を示すことに注力する。

なので外部ライブラリを使ったり汎用性の高い組み方をしたりというのは一切考えない。

## 目標とする挙動

- AND回路が実現する
- XOR回路が実現する

## ニューラルネットワークの構成

- 前項の目標とする挙動を実現するためには2層で十分であろうと見越し、2層構造とする
- 1層あたり2節点とする
- ミニバッチのサンプル数を3とする
- 活性化関数は単純な非正規化線形関数 (`f(u) = max(u,0)`)とする
- バイアスを重みに組み込む方法と分離する方法があるが、今回はわかりやすさのために分離する
  分離したほうが計算コスト低い。
- 学習方法は確率的勾配法を使用する。
- 非正規化関数との相性から、(-1, +1)系、出力は(0, 1)系とする

## プログラムの構成
- `Matrix` : 行列演算関係。使うものしか実装しない。
- `ActivationFunction` : 活性化関数関係。導関数も必要。行列を引数とできるようにしている。
- `Layer` : ニューラルネットワークの1層に対応。最初の層と最後の層は特殊な処理をするため、子クラスとして定義。
- `main.cpp` : entry point
- 試行回数はとりあえず64回。経験値。

## 実行結果

```
u = [
   +1    -1    +1
   -1    +1    +1
]
```

### AND回路
```
weight
0.500000 0.500000
0.178112 0.178112
bias
+0.000000
-0.321888
```
```
Wu + b = [
      0     0     1
  -0.32 -0.32 0.036
}
```
出力の第0成分が入力の2成分のAND演算となる。

### XOR回路
```
weight
-0.385788 -0.385788
 0.16706   0.16706
bias
0.885788
-0.33294
```
```
Wu + b = [
   0.89  0.89 0.116
  -0.33 -0.33 0.001
}
```
出力の第0成分が概ね入力の2成分のXOR演算となる。

## まとめ
AND回路とXOR回路で挙動を確認。
XOR回路で学習の精度が低いのは、2層で計算をしているために自由度が低すぎるためではないかと予想。
次回の目標は、3層での計算を試してみる。
また、自己符号化器に向けて恒等写像が実現できるかも確認する。

